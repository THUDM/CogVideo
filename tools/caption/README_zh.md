# è§†é¢‘Caption

é€šå¸¸ï¼Œå¤§å¤šæ•°è§†é¢‘æ•°æ®ä¸å¸¦æœ‰ç›¸åº”çš„æè¿°æ€§æ–‡æœ¬ï¼Œå› æ­¤éœ€è¦å°†è§†é¢‘æ•°æ®è½¬æ¢ä¸ºæ–‡æœ¬æè¿°ï¼Œä»¥æä¾›å¿…è¦çš„è®­ç»ƒæ•°æ®ç”¨äºæ–‡æœ¬åˆ°è§†é¢‘æ¨¡å‹ã€‚

## é¡¹ç›®æ›´æ–°
- ğŸ”¥ğŸ”¥ **News**: ```2024/9/19```: CogVideoX è®­ç»ƒè¿‡ç¨‹ä¸­ç”¨äºå°†è§†é¢‘æ•°æ®è½¬æ¢ä¸ºæ–‡æœ¬æè¿°çš„ Caption
  æ¨¡å‹ [CogVLM2-Caption](https://huggingface.co/THUDM/cogvlm2-llama3-caption)
  å·²ç»å¼€æºã€‚æ¬¢è¿å‰å¾€ä¸‹è½½å¹¶ä½¿ç”¨ã€‚

## é€šè¿‡ CogVLM2-Caption æ¨¡å‹ç”Ÿæˆè§†é¢‘Caption

ğŸ¤— [Hugging Face](https://huggingface.co/THUDM/cogvlm2-llama3-caption) | ğŸ¤– [ModelScope](https://modelscope.cn/models/ZhipuAI/cogvlm2-llama3-caption/)

CogVLM2-Captionæ˜¯ç”¨äºç”ŸæˆCogVideoXæ¨¡å‹è®­ç»ƒæ•°æ®çš„è§†é¢‘captionæ¨¡å‹ã€‚

### å®‰è£…ä¾èµ–
```shell
pip install -r requirements.txt
```

### è¿è¡Œcaptionæ¨¡å‹

```shell
python video_caption.py
```

ç¤ºä¾‹ï¼š
<div align="center">
    <img width="600px" height="auto" src="./assests/CogVLM2-Caption-example.png">
</div>

## é€šè¿‡ CogVLM2-Video æ¨¡å‹ç”Ÿæˆè§†é¢‘Caption

[Code](https://github.com/THUDM/CogVLM2/tree/main/video_demo) | ğŸ¤— [Hugging Face](https://huggingface.co/THUDM/cogvlm2-video-llama3-chat) | ğŸ¤– [ModelScope](https://modelscope.cn/models/ZhipuAI/cogvlm2-video-llama3-chat) | ğŸ“‘ [Blog](https://cogvlm2-video.github.io/) ï½œ [ğŸ’¬ Online Demo](http://cogvlm2-online.cogviewai.cn:7868/)

CogVLM2-Video æ˜¯ä¸€ä¸ªå¤šåŠŸèƒ½çš„è§†é¢‘ç†è§£æ¨¡å‹ï¼Œå…·å¤‡åŸºäºæ—¶é—´æˆ³çš„é—®é¢˜å›ç­”èƒ½åŠ›ã€‚ç”¨æˆ·å¯ä»¥è¾“å…¥è¯¸å¦‚ `Describe this video in detail.` çš„æç¤ºè¯­ç»™æ¨¡å‹ï¼Œä»¥è·å¾—è¯¦ç»†çš„è§†é¢‘Captionï¼š


<div align="center">
    <a href="https://cogvlm2-video.github.io/"><img width="600px" height="auto" src="./assests/cogvlm2-video-example.png"></a>
</div>

ç”¨æˆ·å¯ä»¥ä½¿ç”¨æä¾›çš„[ä»£ç ](https://github.com/THUDM/CogVLM2/tree/main/video_demo)åŠ è½½æ¨¡å‹æˆ–é…ç½® RESTful API æ¥ç”Ÿæˆè§†é¢‘Captionã€‚


## Citation

ğŸŒŸ If you find our work helpful, please leave us a star and cite our paper.

CogVLM2-Caption:
```
@article{yang2024cogvideox,
  title={CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer},
  author={Yang, Zhuoyi and Teng, Jiayan and Zheng, Wendi and Ding, Ming and Huang, Shiyu and Xu, Jiazheng and Yang, Yuanming and Hong, Wenyi and Zhang, Xiaohan and Feng, Guanyu and others},
  journal={arXiv preprint arXiv:2408.06072},
  year={2024}
}
```
CogVLM2-Video:
```
@article{hong2024cogvlm2,
  title={CogVLM2: Visual Language Models for Image and Video Understanding},
  author={Hong, Wenyi and Wang, Weihan and Ding, Ming and Yu, Wenmeng and Lv, Qingsong and Wang, Yan and Cheng, Yean and Huang, Shiyu and Ji, Junhui and Xue, Zhao and others},
  journal={arXiv preprint arXiv:2408.16500},
  year={2024}
}
```
