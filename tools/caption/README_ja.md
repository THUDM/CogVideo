# ãƒ“ãƒ‡ã‚ªã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³

é€šå¸¸ã€ã»ã¨ã‚“ã©ã®ãƒ“ãƒ‡ã‚ªãƒ‡ãƒ¼ã‚¿ã«ã¯å¯¾å¿œã™ã‚‹èª¬æ˜æ–‡ãŒä»˜ã„ã¦ã„ãªã„ãŸã‚ã€ãƒ“ãƒ‡ã‚ªãƒ‡ãƒ¼ã‚¿ã‚’ãƒ†ã‚­ã‚¹ãƒˆã®èª¬æ˜ã«å¤‰æ›ã—ã¦ã€ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ãƒ“ãƒ‡ã‚ªã¸ã®ãƒ¢ãƒ‡ãƒ«ã«å¿…è¦ãªãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’æä¾›ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

## æ›´æ–°ã¨ãƒ‹ãƒ¥ãƒ¼ã‚¹
- ğŸ”¥ğŸ”¥ **ãƒ‹ãƒ¥ãƒ¼ã‚¹**: ```2024/9/19```ï¼šCogVideoX
  ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ—ãƒ­ã‚»ã‚¹ã§ã€ãƒ“ãƒ‡ã‚ªãƒ‡ãƒ¼ã‚¿ã‚’ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›ã™ã‚‹ãŸã‚ã®ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ãƒ¢ãƒ‡ãƒ« [CogVLM2-Caption](https://huggingface.co/THUDM/cogvlm2-llama3-caption)
  ãŒã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹åŒ–ã•ã‚Œã¾ã—ãŸã€‚ãœã²ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ã”åˆ©ç”¨ãã ã•ã„ã€‚
## CogVLM2-Captionã«ã‚ˆã‚‹ãƒ“ãƒ‡ã‚ªã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³

ğŸ¤— [Hugging Face](https://huggingface.co/THUDM/cogvlm2-llama3-caption) | ğŸ¤– [ModelScope](https://modelscope.cn/models/ZhipuAI/cogvlm2-llama3-caption/)

CogVLM2-Captionã¯ã€CogVideoXãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã‚‹ãƒ“ãƒ‡ã‚ªã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚

### ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
```shell
pip install -r requirements.txt
```

### ä½¿ç”¨æ–¹æ³•
```shell
python video_caption.py
```

ä¾‹:
<div align="center">
    <img width="600px" height="auto" src="./assests/CogVLM2-Caption-example.png">
</div>



## CogVLM2-Video ã‚’ä½¿ç”¨ã—ãŸãƒ“ãƒ‡ã‚ªã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³

[Code](https://github.com/THUDM/CogVLM2/tree/main/video_demo) | ğŸ¤— [Hugging Face](https://huggingface.co/THUDM/cogvlm2-video-llama3-chat) | ğŸ¤– [ModelScope](https://modelscope.cn/models/ZhipuAI/cogvlm2-video-llama3-chat) | ğŸ“‘ [Blog](https://cogvlm2-video.github.io/) ï½œ [ğŸ’¬ Online Demo](http://cogvlm2-online.cogviewai.cn:7868/)


CogVLM2-Video ã¯ã€ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãƒ™ãƒ¼ã‚¹ã®è³ªå•å¿œç­”æ©Ÿèƒ½ã‚’å‚™ãˆãŸå¤šæ©Ÿèƒ½ãªãƒ“ãƒ‡ã‚ªç†è§£ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯ `ã“ã®ãƒ“ãƒ‡ã‚ªã‚’è©³ç´°ã«èª¬æ˜ã—ã¦ãã ã•ã„ã€‚` ãªã©ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒ¢ãƒ‡ãƒ«ã«å…¥åŠ›ã—ã¦ã€è©³ç´°ãªãƒ“ãƒ‡ã‚ªã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’å–å¾—ã§ãã¾ã™ï¼š
<div align="center">
    <a href="https://cogvlm2-video.github.io/"><img width="600px" height="auto" src="./assests/cogvlm2-video-example.png"></a>
</div>

ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯æä¾›ã•ã‚ŒãŸ[ã‚³ãƒ¼ãƒ‰](https://github.com/THUDM/CogVLM2/tree/main/video_demo)ã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã‹ã€RESTful API ã‚’æ§‹æˆã—ã¦ãƒ“ãƒ‡ã‚ªã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆã§ãã¾ã™ã€‚

## Citation

ğŸŒŸ If you find our work helpful, please leave us a star and cite our paper.

CogVLM2-Caption:
```
@article{yang2024cogvideox,
  title={CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer},
  author={Yang, Zhuoyi and Teng, Jiayan and Zheng, Wendi and Ding, Ming and Huang, Shiyu and Xu, Jiazheng and Yang, Yuanming and Hong, Wenyi and Zhang, Xiaohan and Feng, Guanyu and others},
  journal={arXiv preprint arXiv:2408.06072},
  year={2024}
}
```
CogVLM2-Video:
```
@article{hong2024cogvlm2,
  title={CogVLM2: Visual Language Models for Image and Video Understanding},
  author={Hong, Wenyi and Wang, Weihan and Ding, Ming and Yu, Wenmeng and Lv, Qingsong and Wang, Yan and Cheng, Yean and Huang, Shiyu and Ji, Junhui and Xue, Zhao and others},
  journal={arXiv preprint arXiv:2408.16500},
  year={2024}
}
```
